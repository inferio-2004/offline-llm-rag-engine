# offline-llm-rag-engine
Offline Python-based LLM RAG engine using Flask and llama-cpp, supporting LLaMA, Phi, and Falcon with FAISS/BM25 hybrid retrieval. Includes session-aware context handling, LRU-cached multi-instance orchestration
